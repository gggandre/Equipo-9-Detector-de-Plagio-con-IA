The integration of empathy into healthcare chatbots holds promise for instilling a sense of human warmth. Nevertheless, existing research often neglects the multidimensionality of empathy, leading to an incomplete understanding of whether artificial empathy is perceived similarly to interpersonal empathy. This paper posits that implementing experiential expressions of empathy may yield unintended negative consequences, as they could be perceived as inauthentic. Instead, offering instrumental support could prove more appropriate for modeling artificial empathy, aligning better with computer-like schemas conducive to chatbot interactions. Two experimental studies employing healthcare chatbots investigate the impact of empathetic (feeling with), sympathetic (feeling for), and behavioral-empathetic (empathetic helping) versus non-empathetic responses on perceived warmth, perceived authenticity, and their consequent effects on trust and usage intentions. Results indicate that any form of empathy (versus no empathy) enhances perceived warmth, resulting in heightened trust and usage intentions. As hypothesized, empathetic and sympathetic responses diminish the chatbot's perceived authenticity, thereby attenuating this positive effect in both studies. However, a third study fails to replicate this backfiring effect in human-human interactions. This research thus highlights the notion that empathy does not uniformly apply to human-bot interactions. Furthermore, it introduces the concept of ‘perceived authenticity’ and illustrates that distinctively human attributes might backfire by feeling inauthentic in interactions with chatbots.