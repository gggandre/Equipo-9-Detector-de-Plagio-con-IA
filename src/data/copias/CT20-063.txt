Artificial neural networks have emerged as plausible models of human language processing. A key critique of these models is the vast amount of training data they receive, surpassing human exposure during language learning. This study employs two approaches to assess how the models' ability to capture human fMRI responses to sentences is affected by the training data volume. Firstly, GPT-2 models trained on varying word counts—1 million, 10 million, 100 million, or 1 billion—are evaluated against an fMRI benchmark. The 100-million-word model is considered developmentally plausible, akin to children's early-life language exposure. Secondly, the performance of a GPT-2 model trained on a 9-billion-token dataset is examined during training stages to achieve state-of-the-art next-word prediction performance on the human benchmark. Across both approaches, it is found that models trained on a developmentally plausible data volume already achieve near-maximal performance in capturing fMRI responses to sentences. Moreover, lower perplexity—indicating higher next-word prediction performance—is associated with stronger alignment with human data, implying that sufficiently trained models acquire representations predictive of human fMRI responses. These findings underscore the importance of training data volume in the models' predictive ability, suggesting that a developmentally realistic amount of training (~100 million words) may be adequate.