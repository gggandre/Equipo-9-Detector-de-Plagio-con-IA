From ELIZA to Alexa, Conversational Agents (CAs) have been meticulously designed to evoke or simulate empathy. While empathy can enhance technology's efficacy in meeting human needs, it also poses ethical challenges concerning deception and potential exploitation. This work endeavors to characterize empathy in CA interactions, emphasizing the imperative of distinguishing empathy between humans and between humans and CAs. By systematically prompting CAs backed by large language models (LLMs) to exhibit empathy across diverse human identities, this study unveils value judgments inherent in certain identities, highlighting potential encouragement of harmful ideologies by CAs. Furthermore, computational approaches to empathy elucidate CAs' shortcomings in interpreting and exploring user experiences, underscoring the disparity vis-a-vis their human counterparts.