Multimodal Emotion Recognition in Conversations (ERC) seeks to identify emotions conveyed by each utterance in conversational videos. Existing efforts grapple with balancing intra- and inter-speaker context dependencies when tackling intra-modal interactions, crucial for modeling self-dependency (emotional inertia) and interpersonal dependencies (empathy). Additionally, addressing cross-modal interactions involving content with conflicting emotions across different modalities poses challenges. To tackle these issues, an adaptive interactive graph network (IGN) called AdaIGN, leveraging the Gumbel Softmax trick for adaptive node and edge selection, is introduced. Unlike undirected graphs, a directed IGN is utilized to prevent future utterances from impacting the current one. Node- and Edge-level Selection Policies (NESP) guide node and edge selection, alongside a Graph-Level Selection Policy (GSP) integrating original IGN and NESP-enhanced IGN utterance representations. A task-specific loss function prioritizing text modality and intra-speaker context selection is designed. To reduce computational complexity, predefined pseudo labels via self-supervised methods mask unnecessary utterance nodes for selection. Experimental results demonstrate AdaIGN outperforming state-of-the-art methods on two popular datasets. The code will be available at https://github.com/TuGengs/AdaIGN.