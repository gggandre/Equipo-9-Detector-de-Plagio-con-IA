-----------------------------------------------------
Archivo original: As of 2021, more than 30 countries have released national artificial intelligence (AI) policy strategies. These documents articulate plans and expectations regarding how AI will impact policy sectors, including education, and typically discuss the social and ethical implications of AI. This article engages in thematic analysis of 24 such national AI policy strategies, reviewing the role of education in global AI policy discourse. It finds that the use of AI in education (AIED) is largely absent from policy conversations, while the instrumental value of education in supporting an AI-ready workforce and training more AI experts is overwhelmingly prioritized. Further, the ethical implications of AIED receive scant attention despite the prominence of AI ethics discussion generally in these documents. This suggests that AIED and its broader policy and ethical implicationsâgood or badâhave failed to reach mainstream awareness and the agendas of key decision-makers, a concern given that effective policy and careful consideration of ethics are inextricably linked, as this article argues. In light of these findings, the article applies a framework of five AI ethics principles to consider ways in which policymakers can better incorporate AIEDâs implications. Finally, the article offers recommendations for AIED scholars on strategies for engagement with the policymaking process, and for performing ethics and policy-oriented AIED research to that end, in order to shape policy deliberations on behalf of the public good.

Archivo de copia: Network news serves as a vital source of social information for netizens, yet the sheer volume of news information often obscures key details. Named entity recognition technology offers a solution by classifying place, date, and other information in text. This article presents an innovative approach that combines named entity recognition and deep learning technology. Our proposed method introduces an automatic annotation approach for Chinese entity triggers and a Named Entity Recognition (NER) model capable of achieving high accuracy with a small training dataset. By jointly training sentence and trigger vectors through a trigger-matching network, our model effectively recognizes neologisms in web news and extends the web news word sentiment lexicon for sentiment observation. Experimental results demonstrate superior performance compared to traditional models, achieving an average accuracy rate of 97.88% in sentiment viewpoint detection.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: As addressed by Stephen Yang in his ICCE 2019 keynote speech (Yang, 2019), precision
education is a new challenge when applying artificial intelligence (AI), machine learning, and learning analytics
to improve teaching quality and learning performance. The goal of precision education is to identify at-risk
students as early as possible and provide timely intervention on the basis of teaching and learning experiences
(Lu et al., 2018). Drawing from this main theme of precision education, this special issue advocates an in-depth
dialogue between cold technology and warm humanity, in turn offering greater understanding of precision
education. For this special issue, thirteen research papers that specialize in precision education, AI, machine
learning, and learning analytics to engage in an in-depth research experiences concerning various applications,
methods, pedagogical models, and environments were exchanged to achieve better understanding of the
application of AI in education

Archivo de copia: As the use of Artificial Intelligence (AI) technologies in education continues to rise, there has been a notable increase in published studies in this field. However, no large-scale reviews have been undertaken to comprehensively explore the various aspects of AI applications in education. By analyzing 4,519 publications spanning from 2000 to 2019, we aim to address this gap and identify trends and topics related to AI applications in education (AIEd) using topic-based bibliometrics. Our review results reveal a growing interest in utilizing AI for educational purposes within the academic community. Key research topics include intelligent tutoring systems for special education, natural language processing for language education, educational robots for AI education, educational data mining for performance prediction, discourse analysis in computer-supported collaborative learning, neural networks for teaching evaluation, affective computing for learner emotion detection, and recommender systems for personalized learning. We also discuss the challenges and future directions of AIEd.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 87.44%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: With the increasing use of Artificial Intelligence (AI) technologies in education, the number of published studies in the field has increased. However, no large-scale reviews have been conducted to comprehensively investigate the various aspects of this field. Based on 4,519 publications from 2000 to 2019, we attempt to fill this gap and identify trends and topics related to AI applications in education (AIEd) using topicbased bibliometrics. Results of the review reveal an increasing interest in using AI for educational purposes from the academic community. The main research topics include intelligent tutoring systems for special education; natural language processing for language education; educational robots for AI education; educational data mining for performance prediction; discourse analysis in computer-supported collaborative learning; neural networks for teaching evaluation; affective computing for learner emotion detection; and recommender systems for personalized learning. We also discuss the challenges and future directions of AIEd.

Archivo de copia: The application of AI and machine learning, particularly the vision transformer method, in bacterial detection offers a promising solution to address the limitations of traditional methods. This research introduces a novel positional self-attention transformer model for the classification of bacterial colonies, presenting a groundbreaking approach to overcoming existing challenges. Leveraging the success of transformer architectures across various domains, we have enhanced the model's performance by integrating a positional self-attention mechanism. Our proposed approach allows for the effective capture of spatial relationships and patterns within bacterial colonies, leading to highly accurate classification results. Trained on a substantial dataset of bacterial images, our model demonstrates robustness and generalization to diverse colony types. With an accuracy rate of 98.50% in colony classification, our model surpasses traditional methods, offering unprecedented accuracy in discerning subtle morphological variations. The adaptability of our model to diverse colony shapes and arrangements marks a significant advancement, promising to redefine bacterial colony analysis through the lens of state-of-the-art deep learning techniques.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 85.76%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Artificial intelligence (AI) is rapidly transforming various industries, including education. AI is being used in educational management
to enhance the learning process, improve student outcomes, and streamline administrative tasks. This research work aims to explore the
application of AI in educational management, including its benefits and challenges. The research work employs a systematic review
methodology, examining the literature on AI in educational management. The study finds that AI has several advantages, including
improving student engagement, personalization of learning, and cost-effectiveness. However, AI also poses several challenges, such as
ethical concerns, potential biases, and the need for re-skilling the workforce. The research concludes that AI has an enormous capacity
to improve educational management, but it must be deployed with care and caution.

Archivo de copia: Facial recognition technology (FRT) stands as one of the most successful and captivating technologies of modern times, particularly in the wake of the COVID-19 pandemic. The adoption of contactless FRT is gaining momentum globally due to its biometric characteristics and contactless nature. Businesses are increasingly turning to AI-based FRT, replacing conventional fingerprint scanners and unlocking vast commercial opportunities. With applications spanning security and surveillance, authentication/access control systems, digital healthcare, and photo retrieval, FRT has become indispensable across various sectors. This communication presents a global overview of FRT adoption, its market trends, utilization in diverse sectors, as well as challenges and emerging concerns, with a focus on India and worldwide.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Agriculture is the ultimate imperative and primary source of origin to furnish domestic income for multifarious countries. The disease caused in plants due to various pathogens like viruses, fungi, and bacteria is liable for considerable monetary losses in the agriculture corporation across the world. The security of crops concerning quality and quantity is crucial to monitor disease in plants. Thus, recognition of plant disease is essential. The plant disease syndrome is noticeable in distinct parts of plants. Nonetheless, commonly the infection is detected in distinct leaves of plants. Computer vision, deep learning, few-shot learning, and soft computing techniques are utilized by various investigators to automatically identify the disease in plants via leaf images. These techniques also benefit farmers in achieving expeditious and appropriate actions to avoid a reduction in the quality and quantity of crops. The application of these techniques in the recognition of disease can avert the disadvantage of origin by a factious selection of disease features, extraction of features, and boost the speed of technology and efficiency of research. Also, certain molecular techniques have been established to prevent and mitigate the pathogenic threat. Hence, this review helps the investigator to automatically detect disease in plants using machine learning, deep learning and few shot learning and provide certain diagnosis techniques to prevent disease. Moreover, some of the future works in the classification of disease are also discussed.
Archivo de copia: Over recent decades, the proliferation of vehicles on roads has steadily increased, driven by the growing demand for urban mobility and modern logistics. However, this surge in vehicle numbers has led to detrimental effects such as increased traffic congestion and traffic accidents, hindering economic development. Addressing these challenges requires making vehicles smarter and less reliant on human intervention. Extensive research conducted globally over the past century has fueled the automation of road vehicles, with significant contributions from major motor manufacturers worldwide. The development of autonomous vehicle (AV) technologies is currently underway, propelled by advancements in artificial intelligence (AI) and big data. Understanding AI's development history is crucial to comprehend its role in AV systems, enabling vehicles to perceive their surroundings and make real-time decisions autonomously.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: The application of AI and machine learning, particularly the vision transformer method, in bacterial detection presents a promising solution to overcome limitations of traditional methods, offering faster and more accurate detection of disease-causing bacteria like E. coli and salmonella in water, crucial for human survival, with ongoing research to further assess its effectiveness in microbiology. This research introduces a revolutionary positional self-attention transformer model for the classification of bacterial colonies. Leveraging the proven success of transformer architectures in various domains, we enhanced the model's performance by integrating a positional self-attention mechanism. We presented a novel approach for bacterial colony classification utilizing a positional self-attention transformer model. This allows the model to effectively capture spatial relationships and patterns within bacterial colonies, contributing to highly accurate classification results. We trained the model on a substantial dataset of bacterial images, which ensures its robustness and generalization to diverse colony types. The proposed model adeptly captured the spatial relationships and sequential patterns inherent in bacterial colony images, allowing for more accurate and robust classification. The proposed model demonstrated remarkable performance, achieving an accuracy of 98.50% in the classification of bacterial colonies. This novel approach surpasses traditional methods by effectively capturing intricate spatial relationships within microbial structures, offering unprecedented accuracy in discerning subtle morphological variations. The model's adaptability to diverse colony shapes and arrangements marks a significant advancement, promising to redefine the landscape of bacterial colony classification through the lens of state-of-the-art deep learning techniques. The high classification accuracy attained by the model, suggests its potential for practical applications in the early diagnosis of infectious diseases and the development of targeted treatments. The findings of this study underscore the effectiveness of incorporating positional self-attention in transformer models for image-based classification tasks, particularly in the domain of bacterial colony analysis.
Archivo de copia: As AI systems increasingly shape various aspects of society, concerns regarding AI bias and its impact on underprivileged groups become more prominent. Drawing from prior research demonstrating Virtual Reality's ability to enhance empathy, this laboratory study investigates participants' reactions to a biased AI scenario while embodying different personas. Participants embodied personas with varying abilities to achieve high financial credit scores due to age and gender, while interacting with a biased AI in Virtual Reality. Results indicate that participants felt significantly more empathy toward embodied personas and perceived the AI as less fair compared to a baseline condition. Qualitative analysis sheds light on participants' mental model creation concerning embodied personas.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: The face is the most essential part of the human body, and because of its distinctive traits, it is crucial for recognizing people. Facial recognition technology (FRT) is one of the most successful and fascinating technologies of the modern times. The world is moving towards contactless FRT after the COVID-19 pandemic. Due to its contactless biometric characteristics, FRT is becoming quite popular worldwide. Businesses are replacing conventional fingerprint scanners with artificial intelligenceâbased FRT, opening up enormous commercial prospects. Security and surveillance, authentication/access control systems, digital healthcare, photo retrieval, etc., are some sectors where its use has become essential. In the present communication, we presented the global adoption of FRT, its rising trend in the market, utilization of the technology in various sectors, its challenges and rising concerns with special reference to India and worldwide.
Archivo de copia: Purpose: This study aims to investigate the effectiveness of recommender systems in knowledge discovery.

Methodology: The study adopts a desktop research methodology, relying on secondary data collected from existing resources. Through systematic literature review, the study explores the effectiveness of recommender systems in knowledge discovery, emphasizing the role of advanced algorithms and personalized techniques in generating relevant recommendations. Contextual information is identified as a crucial factor influencing recommendation effectiveness.

Findings: The study reveals a contextual and methodological gap in recommender systems' effectiveness in knowledge discovery. It highlights the positive correlation between user engagement metrics and knowledge discovery outcomes, emphasizing the importance of continuous refinement of recommender system algorithms. Recommendations include adopting hybrid recommender systems, integrating contextual information, and prioritizing user engagement to enhance knowledge discovery outcomes.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 77.86%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Recent technological developments have enabled computers to identify and categorize facial expressions to determine a personâs emotional state in an image or a video. This process, called âFacial Expression Recognition (FER)â, has become one of the most popular research areas in computer vision. In recent times, deep FER systems have primarily concentrated on addressing two significant challenges: the problem of overfitting due to limited training data availability, and the presence of expression-unrelated variations, including illumination, head pose, image resolution, and identity bias. In this paper, a comprehensive survey is provided on deep FER, encompassing algorithms and datasets that offer insights into these intrinsic problems. Initially, this paper presents a detailed timeline showcasing the evolution of methods and datasets in deep facial expression recognition (FER). This timeline illustrates the progression and development of the techniques and data resources used in FER. Then, a comprehensive review of FER methods is introduced, including the basic principles of FER (components such as preprocessing, feature extraction and classification, and methods, etc.) from the pro-deep learning era (traditional methods using handcrafted features, i.e., SVM and HOG, etc.) to the deep learning era. Moreover, a brief introduction is provided related to the benchmark datasets (there are two categories: controlled environments (lab) and uncontrolled environments (in the wild)) used to evaluate different FER methods and a comparison of different FER models. Existing deep neural networks and related training strategies designed for FER, based on static images and dynamic image sequences, are discussed. The remaining challenges and corresponding opportunities in FER and the future directions for designing robust deep FER systems are also pinpointed.
Archivo de copia: Monte Carlo Tree Search (MCTS) stands as an empirical search algorithm widely used in agent decision-making, particularly when combined with Deep Learning (DL), to master board games once deemed unconquerable. However, its success in real-time video games, which have stringent simulation time limits for exploration due to their design for human players, is less evident. This paper introduces a surrogate-assisted MCTS approach tailored for commercial real-time video games by approximating gameplay outcomes with a deep-learning-based surrogate model. The main contribution lies in designing a modified MCTS specifically for commercial real-time video games, which are significantly more complex and dynamic than non-commercial counterparts. We validated our approach through comparative experiments with other algorithms, including traditional MCTS, within the context of a commercial real-time video game environment.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Face recognition has a very important role in various applications, from security, surveillance to authentication. For safety most of the household is having CC cameras such that they could recognize the persons from it. These CCTV are allocated for having safety and to know who visited their houses. In few highly secured places where allowance to any unknown intruder is strictly prohibited. Thereby, this paper deals with a system which could recognize the face of the intruder through surveillance camera using ML and AI based algorithms. The design specified is successfully implemented using HOG feature extraction and SVM classification algorithms and it classifies the faces for a video stream given as input. The major objective entitled to this paper is to recognize the faces of people from the video by HOG feature extractor and classify them using SVM and train the machine to tell who is the person working for the organization and who are the intruder.
Archivo de copia: Fuzzy inference systems (FISs) have been developed over many years, yet addressing high-dimensional problems remains a challenge. The product T-norm, commonly used for computing firing strengths, often suffers from numeric underflow issues, especially in high-dimensional scenarios. This paper focuses on mitigating this problem in high-dimensional FISs (HDFISs). For the product T-norm, we propose HDFIS-prod, which circumvents numeric underflow problems by incorporating an adaptive dimension-dependent membership function (DMF). Additionally, we address challenges with the minimum T-norm, resulting in another HDFIS named HDFIS-min. Both HDFIS-prod and HDFIS-min are evaluated on datasets with varying feature dimensions, demonstrating competitive performance on high-dimensional datasets.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Accurate trajectory tracking is unrealistic in real-world scenarios, however, which is commonly assumed to facilitate motion planning algorithm design. In this paper, a safe and reliable motion planning and control framework is proposed to handle the tracking errors caused by inaccurate tracking by coordinating the motion planning layer and controller. Specifically, motion space is divided into safe regions and risky regions by designing the movement restraint size dependent on tracking error to construct the repulsive potential field. The collision-free waypoint set can then be obtained by combining global search and the proposed waypoint set filtering method. The planned trajectory is fitted by an optimization-based approach which minimizes the acceleration of the reference trajectory. Then, the planned trajectory is checked and modified by the designed anti-collision modification to ensure safety. Using invertible transformation and adaptive compensation allows the transient trajectory tracking errors to be limited within the designed region even with actuator faults. Because tracking error is considered and margined at the planning level, safety and reliability can be guaranteed by the coordination between the planning and control levels under inaccurate tracking and actuator faults. The advantages and effectiveness of the proposed motion planning and control method are verified by simulation and experimental results.
Archivo de copia: Artificial intelligence (AI) has emerged as a transformative force across various sectors, including medicine and healthcare. Models like ChatGPT exemplify AI's potential by generating human-like text. ChatGPT's versatility holds promise for reshaping medical practices, improving patient care, and enhancing interactions within the healthcare ecosystem. In pandemic management, ChatGPT serves as a virtual assistant, aids in surgical consultations, simplifies medical education, and facilitates disease diagnosis. This study categorizes 82 papers into eight areas, exploring AI's transformative potential in healthcare. Despite its potential, balancing AI's role with human judgment remains a challenge. A systematic literature review highlights ChatGPT's diverse medical applications, limitations, and future challenges, serving as a valuable resource for healthcare stakeholders.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Throughout the last decades, the number of vehicles on the road has steadily increased due to the rising demand for urban mobility and contemporary logistics. Two of the many detrimental effects of more vehicles on the road, which also impede economic development, are increased traffic congestion and traffic accidents. The issues mentioned above can be significantly resolved by making vehicles smarter by reducing their reliance on humans. Over the past century, various nations have conducted extensive research that has fueled the automation of road vehicles. The development of autonomous vehicle (AV) technologies is currently being pursued by all significant motor manufacturers worldwide. Undoubtedly, the widespread use of autonomous cars is more imminent than we realize given the development of artificial intelligence (AI). In order for AVs to perceive their surroundings and make the right decisions in real time, AI has emerged as a crucial component. This development of AI is being driven by the growth of big data from numerous sensing devices and cutting-edge computing resources. We must first examine AI's development and history in order to comprehend its functions in AV systems.
Archivo de copia: The rise of autonomous vehicles (AVs) heralds a transformative era in transportation, driven by the integration of Artificial Intelligence (AI) and learning algorithms. This paper comprehensively explores AI's evolution within AVs, tracing its journey from foundational principles to recent advancements. It delves into AI's role in shaping autonomous decision-making capabilities, addressing ethical considerations and bias in AI-driven software development. Statistical insights into AI usage within the automotive industry are presented, along with discussions on algorithm refinement and autonomy levels. Additionally, the paper examines software package sizes across autonomy levels, highlighting AI's nuanced usage and automation of key tasks.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: The future sustainability of the global automotive industry will be greatly affected by the fourth industrial revolution and the evolution of artificial intelligence (AI). The ânew normalâ is projected to be driven by new industry standards including an increasingly autonomous self-driving technology, amended safety standards, more complex insurance regulations, adaptive social resistance to technological change, city infrastructure requirements with a digital divide, and disruptive business innovation based on strategic input supply partnerships with open-source AI. In this chapter, the key factors of the autonomous vehicles (AVs) are analyzed using AI developments in radar and laser technology, commercial risk factors, self-driving consumer behavior, city infrastructure constraints, and social adaptations to new technology. The future trajectory of the AV industry is expected to be an interplay between commercial, social, risk, infrastructure, and regulatory mechanisms with various impacts on the industryâs stakeholders. This study predicts that the most likely sustainable scenario for the AV industry is that it will be driven by: (1) AIâs pulsed laser LiDAR (Light Detection and Ranging) with a sufficient loop frequency and GPS bi-directional cloud technology requirement, (2) pooled insurance in contrast to individual liability, (3) smart city infrastructure with expected sharp digital divide across transport regions leading to more regional inequality, and (4) customers who strongly prefer a human controlled semi-autonomous vehicle rather than complete machine autonomy.
Archivo de copia: Face recognition plays a crucial role in security applications, particularly in household CC cameras aimed at recognizing individuals. This paper presents a system utilizing machine learning (ML) and AI algorithms to recognize intruders through surveillance cameras. By implementing HOG feature extraction and SVM classification algorithms, the system successfully classifies faces from video streams. The primary objective is to recognize individuals using HOG feature extraction and SVM, distinguishing between authorized personnel and intruders within an organization.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolution is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statistical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry. Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level. Additionally, the document discusses the variation in software package sizes across different autonomy levels
Archivo de copia: Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agentsâ empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definitionâan ontologyâof empathy is developed. We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactionsâbe it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors.
¿Es plagio?: No
Tipo de plagio: Parafraseo
Porcentaje de plagio: 100.0%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agentsâ empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definitionâan ontologyâof empathy is developed. We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactionsâbe it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors.

Archivo de copia: With the rapid development of artificial intelligence, there is an increasing number of industries relying on the accuracy and efficiency of deep learning algorithms. However, due to the inexplicability and black box effect of deep neural networks, we can only obtain results without knowing the applied reasoning behind them. Hence derives the skepticism and resistance of some sectors of deep learning-based technologies. In the context of emotion analysis used in businesses and public opinion monitoring, it is sometimes difficult for decision-makers to trust the outcome without explanation from the supposedly emotionless machines. Mathematical-based explanation methods often generalize emotion analysis as a classification task. Nevertheless, emotion should be distinct from other task categories because the generation of emotion involves human-specific factors and logic. This paper proposes an emotion analysis explanation framework that emphasizes considering the cause and trigger of emotions as the explanation for the deep learning-based emotion analysis, comprising two main components: the extraction of the emotion cause and the visualization of emotion-triggering words.
¿Es plagio?: Sí
Tipo de plagio: Parafraseo
Porcentaje de plagio: 94.06%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: In a world increasingly driven by AI systems, controversial use cases for AI that significantly affect peopleâs lives become more likely scenarios. Hence, increasing awareness of AI bias that might affect underprivileged groups becomes an increasing challenge. As Virtual Reality has previously been shown to increase empathy through immersive perspective-taking, we conducted a laboratory study in which participants were confronted with a biased Wizard of Oz AI while embodying personas that varied widely in their ability to achieve high financial credit scores due to their age and gender. We found that participants embodying personas in VR felt significantly more empathy toward the characters they embodied and rated the AI as significantly less fair compared to a baseline condition in which they imagined to be these characters. Furthermore, we investigate differences between embodied personas and discuss qualitative results to gain insight into the participantâs mental model creation.
Archivo de copia: This paper introduces a study employing artificial intelligence (AI) to utilize computer vision algorithms in detecting human emotions in video content during user interactions with diverse visual stimuli. The research aims to unveil the creation of software capable of emotion detection by leveraging AI algorithms and image processing pipelines to identify users' facial expressions. The process involves assessing users through images and facilitating the implementation of computer vision algorithms aligned with psychological theories defining emotions and their recognizable features. The study demonstrates the feasibility of emotion recognition through convolutional neural networks (CNN) and software development and training based on facial expressions. The results highlight successful emotion identification; however, precision improvement requires further training for contexts with more diverse images and additional algorithms to distinguish closely related emotional patterns. The discussion and conclusions emphasize the potential of A.I. and computer vision algorithms in emotion detection, providing insights into software development, ongoing training, and the evolving landscape of emotion recognition technology.
¿Es plagio?: Sí
Tipo de plagio: Parafraseo
Porcentaje de plagio: 96.01%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: With the rapid development of artificial intelligence, there is an increasing number of industries relying on the accuracy and efficiency of deep learning algorithms. But due to the inexplicability and black box effect of deep neural networks, we can only obtain results without knowing the applied reasoning behind them. That engenders scepticism and resistance from some quarters of deep learning-based technologies. In the context of emotion analysis used in business and public opinion monitoring, it is sometimes difficult for decision-makers to trust the outcome without explanation from the supposedly emotionless machines. There are mathematical-based explanation methods, and they often generalise emotion analysis as a classification task. Still, emotion should be different from other task categories because the generation of emotion involves human-specific factors and logic. This paper proposes an emotion analysis explanation framework that is grounded in psychological theories focusing on the stimulus from classic emotion theories. This proposed framework emphasises considering the cause and trigger of emotions as the explanation for the deep learning-based emotion analysis, and it includes two main components: the extraction of the emotion cause and the visualisation of emotion-triggering words.
Archivo de copia: Agriculture is the ultimate imperative and primary source of origin to furnish domestic income for multifarious countries. The disease caused in plants due to various pathogens like viruses, fungi, and bacteria is liable for considerable monetary losses in the agriculture corporation across the world. The security of crops concerning quality and quantity is crucial to monitor disease in plants. Thus, recognition of plant disease is essential. The plant disease syndrome is noticeable in distinct parts of plants. Nonetheless, commonly the infection is detected in distinct leaves of plants. Computer vision, deep learning, few-shot learning, and soft computing techniques are utilized by various investigators to automatically identify the disease in plants via leaf images. These techniques also benefit farmers in achieving expeditious and appropriate actions to avoid a reduction in the quality and quantity of crops. The application of these techniques in the recognition of disease can avert the disadvantage of origin by a factious selection of disease features, extraction of features, and boost the speed of technology and efficiency of research. Also, certain molecular techniques have been established to prevent and mitigate the pathogenic threat. Hence, this review helps the investigator to automatically detect disease in plants using machine learning, deep learning, and few shot learning and provide certain diagnosis techniques to prevent disease. Moreover, some of the future works in the classification of disease are also discussed.
¿Es plagio?: No
Tipo de plagio: Parafraseo
Porcentaje de plagio: 100.0%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: This paper introduces a study employing artificial intelligence (AI) to utilize computer vision algorithms for detecting human emotions in video content during user interactions with diverse visual stimuli. The research aims to unveil the creation of software capable of emotion detection by leveraging AI algorithms and image processing pipelines to identify users' facial expressions. The process involves assessing users through images and facilitating the implementation of computer vision algorithms aligned with psychological theories defining emotions and their recognizable features. The study demonstrates the feasibility of emotion recognition through convolutional neural networks (CNN) and software development and training based on facial expressions. The results highlight successful emotion identification; however, precision improvement necessitates further training for contexts with more diverse images and additional algorithms to distinguish closely related emotional patterns. The discussion and conclusions emphasize the potential of A.I. and computer vision algorithms in emotion detection, providing insights into software development, ongoing training, and the evolving landscape of emotion recognition technology. Further training is necessary for contexts with more diverse images, alongside additional algorithms that can effectively distinguish between facial expressions depicting closely related emotional patterns, enhancing certainty and accuracy.
Archivo de copia: Recent technological developments have enabled computers to identify and categorize facial expressions to determine a personâs emotional state in an image or a video. This process, called âFacial Expression Recognition (FER)â, has become one of the most popular research areas in computer vision. In recent times, deep FER systems have primarily concentrated on addressing two significant challenges: the problem of overfitting due to limited training data availability and the presence of expression-unrelated variations, including illumination, head pose, image resolution, and identity bias. In this paper, a comprehensive survey is provided on deep FER, encompassing algorithms and datasets that offer insights into these intrinsic problems. Initially, this paper presents a detailed timeline showcasing the evolution of methods and datasets in deep facial expression recognition (FER). This timeline illustrates the progression and development of the techniques and data resources used in FER. Then, a comprehensive review of FER methods is introduced, including the basic principles of FER (components such as preprocessing, feature extraction, and classification, and methods, etc.) from the pro-deep learning era (traditional methods using handcrafted features, i.e., SVM and HOG, etc.) to the deep learning era. Moreover, a brief introduction is provided related to the benchmark datasets (there are two categories: controlled environments (lab) and uncontrolled environments (in the wild)) used to evaluate different FER methods and a comparison of different FER models. Existing deep neural networks and related training strategies designed for FER, based on static images and dynamic image sequences, are discussed. The remaining challenges and corresponding opportunities in FER and the future directions for designing robust deep FER systems are also pinpointed.
¿Es plagio?: Sí
Tipo de plagio: Parafraseo
Porcentaje de plagio: 100.0%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: INTRODUCTION: In recent years, there has been a convergence between Artificial Intelligence and neuroscience, particularly in studying the brain and developing treatments for neurological disorders. Artificial neural networks and deep learning provide valuable insights into neural processing and brain functioning. Recent research tries to explain how neural processes influence an individual's happiness. OBJECTIVES: To evaluate the interaction between neuroscience and happiness based on the advances in Artificial Intelligence. METHODS: A bibliometric analysis was performed with articles from the Scopus database in 2013-2023; likewise, the VOSviewer was used for information processing. RESULTS A total of 603 articles were obtained, and it is evident that the most significant scientific production is centered in the United States (184), United Kingdom (74), and China (73). Three clusters are generated from the Co-occurrence - Author Keywords analysis. The first cluster, red, is related to Artificial Intelligence applications for predicting happiness; the second cluster, green, is associated with Artificial Intelligence tools in neuroscience; and the third cluster, blue, is related to neuroscience in psychology. CONCLUSION: Neuroscience research has made significant leaps in understanding mental processes such as emotions and consciousness. Neuroscience has encountered happiness and is opening up to an approach that seeks evidence to understand people's well-being supported by Artificial Intelligence.
Archivo de copia: The future sustainability of the global automotive industry will be greatly affected by the fourth industrial revolution and the evolution of artificial intelligence (AI). The ânew normalâ is projected to be driven by new industry standards including an increasingly autonomous self-driving technology, amended safety standards, more complex insurance regulations, adaptive social resistance to technological change, city infrastructure requirements with a digital divide, and disruptive business innovation based on strategic input supply partnerships with open-source AI. In this chapter, the key factors of the autonomous vehicles (AVs) are analyzed using AI developments in radar and laser technology, commercial risk factors, self-driving consumer behavior, city infrastructure constraints, and social adaptations to new technology. The future trajectory of the AV industry is expected to be an interplay between commercial, social, risk, infrastructure, and regulatory mechanisms with various impacts on the industryâs stakeholders. This study predicts that the most likely sustainable scenario for the AV industry is that it will be driven by: (1) AIâs pulsed laser LiDAR (Light Detection and Ranging) with a sufficient loop frequency and GPS bi-directional cloud technology requirement, (2) pooled insurance in contrast to individual liability, (3) smart city infrastructure with expected sharp digital divide across transport regions leading to more regional inequality, and (4) customers who strongly prefer a human controlled semi-autonomous vehicle rather than complete machine autonomy.
¿Es plagio?: No
Tipo de plagio: Parafraseo
Porcentaje de plagio: 100.0%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Network news is an important way for netizens to get social information. Massive news information hinders netizens to get key information. Named entity recognition technology under artificial background can realize the classification of place, date and other information in text information. This article combines named entity recognition and deep learning technology. Specifically, the proposed method introduces an automatic annotation approach for Chinese entity triggers and a Named Entity Recognition (NER) model that can achieve high accuracy with a small number of training data sets. The method jointly trains sentence and trigger vectors through a trigger-matching network, utilizing the trigger vectors as attention queries for subsequent sequence annotation models. Furthermore, the proposed method employs entity labels to effectively recognize neologisms in web news, enabling the customization of the set of sensitive words and the number of words within the set to be detected, as well as extending the web news word sentiment lexicon for sentiment observation. Experimental results demonstrate that the proposed model outperforms the traditional BiLSTM-CRF model, achieving superior performance with only a 20% proportional training data set compared to the 40% proportional training data set required by the conventional model. Moreover, the loss function curve shows that my model exhibits better accuracy and faster convergence speed than the compared model. Finally, my model achieves an average accuracy rate of 97.88% in sentiment viewpoint detection.

Archivo de copia: This paper substantiates the methodological approach to assessing personnel risks of enterprises by applying the fuzzy logic apparatus to identify personnel risk management issues and provide appropriate recommendations for their resolution. The methodological basis of the study relies on classic provisions and fundamental works of foreign and domestic scientists, statistical data, and the results of research into the problems of assessing personnel risks of enterprises. Fuzzy set theory, comparative analysis, scientific abstraction, generalization of modern theoretical research, and a system-complex approach were employed as methods. The study proposes a methodological approach to assessing the level of personnel risks of an enterprise, and numerical experiments were conducted on a group of construction equipment manufacturers. Analysis of the results of assessing the level of personnel risks of enterprises allowed the identification of problems in managing personnel risks at enterprises. The mathematical problem statement considers hierarchical fuzzy data, with four groups of indicators for assessing the level of personnel risks. These indicators are functions of fuzzy coefficients, and the output variable is an integrated indicator of the personnel risk level, also a fuzzy value. Expert evaluations of input data variation are categorized as Low (I), Medium (G), or High (E). The constructed system of fuzzy logical inference, based on fuzzy set theory, allows for the estimation of the level of personnel risk at the enterprise, facilitating the substantiation of measures to increase its efficiency and providing more efficient decision-making under uncertain conditions.
¿Es plagio?: Sí
Tipo de plagio: Parafraseo
Porcentaje de plagio: 92.44%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Artificial intelligence (AI) has emerged as a transformative force in various sectors, including medicine and healthcare. Large language models like ChatGPT showcase AIâs potential by generating human-like text through prompts. ChatGPTâs adaptability holds promise for reshaping medical practices, improving patient care, and enhancing interactions among healthcare professionals, patients, and data. In pandemic management, ChatGPT rapidly disseminates vital information. It serves as a virtual assistant in surgical consultations, aids dental practices, simplifies medical education, and aids in disease diagnosis. A total of 82 papers were categorised into eight major areas, which are G1: treatment and medicine, G2: buildings and equipment, G3: parts of the human body and areas of the disease, G4: patients, G5: citizens, G6: cellular imaging, radiology, pulse and medical images, G7: doctors and nurses, and G8: tools, devices and administration. Balancing AIâs role with human judgment remains a challenge. A systematic literature review using the PRISMA approach explored AIâs transformative potential in healthcare, highlighting ChatGPTâs versatile applications, limitations, motivation, and challenges. In conclusion, ChatGPTâs diverse medical applications demonstrate its potential for innovation, serving as a valuable resource for students, academics, and researchers in healthcare. Additionally, this study serves as a guide, assisting students, academics, and researchers in the field of medicine and healthcare alike.
Archivo de copia: The swift progress in artificial intelligence (AI) has prompted numerous industries to rely on the precision and efficacy of deep learning algorithms. However, due to the inscrutability and opaque nature of deep neural networks, outcomes are obtained without a clear understanding of the underlying rationale. This lack of transparency has led to skepticism and resistance from some sectors regarding technologies based on deep learning. In domains such as emotion analysis utilized in business and public opinion monitoring, decision-makers sometimes struggle to trust outcomes without explanations from ostensibly emotionless machines. Although mathematical-based explanation methods often categorize emotion analysis as a classification task, it should be noted that emotion differs from other task categories due to the involvement of human-specific factors and logic in its generation. This paper suggests an emotion analysis explanation framework grounded in psychological theories, emphasizing the consideration of emotion cause and trigger as explanations for deep learning-based emotion analysis. The framework comprises two primary components: extracting the emotion cause and visualizing emotion-triggering words.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 77.17%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: The impressive capabilities of living organisms arise from the way autonomy is materialized by their bodies. Across scales, living beings couple computational or cognitive intelligence with physical intelligence through body morphology, material multifunctionality, and mechanical compliance. While soft robotics has advanced the design and fabrication of physically intelligent bodies, the integration of information-processing capabilities for computational intelligence remains a challenge. Consequently, perception and control limitations have constrained how soft robots are built today. Progress toward untethered autonomy will require deliberate convergence in how the field codevelops new materials, fabrication methods, and control strategies for soft robots. Here, a new perspective is put forward: that researchers should use tasks alone to impose material and information constraints on soft robot design. A conceptual framework is proposed for a task-first design paradigm that sidesteps limitations imposed by control strategies. This framework allows emergent synergies between material and information processing properties of soft matter to be readily exploited for task-capable agents. Particular attention is paid to the scale dependence of solutions. Finally, an outlook is presented on emerging research opportunities for achieving autonomy in future soft robots as large as elephant trunks and as small as paramecia.

Archivo de copia: This study introduces the application of artificial intelligence (AI) and computer vision algorithms for detecting human emotions in video content during user interactions with diverse visual stimuli. The research aims to develop software capable of emotion detection by leveraging AI algorithms and image processing pipelines to identify users' facial expressions. The process involves evaluating users through images and implementing computer vision algorithms aligned with psychological theories defining emotions and their recognizable features. The study demonstrates the feasibility of emotion recognition through convolutional neural networks (CNN) and software development and training based on facial expressions. While successful in identifying emotions, further training is needed for contexts with more diverse images and additional algorithms to distinguish closely related emotional patterns. The discussion and conclusions underscore the potential of AI and computer vision algorithms in emotion detection, offering insights into software development, ongoing training, and the evolving landscape of emotion recognition technology.
¿Es plagio?: Sí
Tipo de plagio: Parafraseo
Porcentaje de plagio: 91.55%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: ï»¿In this article, the causes of technological disturbances in electrical systems are considered, and several characteristic disadvantages of the protection and automation of elements of electrical systems are highlighted. The tendency to decrease the reliability of relay protection associated with the transition from analog to digital types of protection is substantiated. Based on the studied examples, the use of fuzzy logic in protections, the expediency of using fuzzy logic elements in protection devices, and the automation of electrical systems to identify types of short circuits are justified. This article analyzes the most common damages and presents the results of modeling an electrical system with transformer coupling, where all types of asymmetric short circuits were initiated. The dynamics of changes in the symmetrical components of short-circuit currents of the forward, reverse, and zero sequences are determined. Rules have been created for the identification of asymmetric types of short circuits. An algorithm of protection and automation operation using fuzzy logic elements has been developed. The proposed algorithm of protection and automation will reduce the time to determine the type of damage and trigger protections.
Archivo de copia: With advancements in education and technology, educators have come to realize that games should not merely serve as a means for student entertainment. Integrating games into teaching resources can yield superior teaching outcomes. However, the internet continually introduces new resources related to this endeavor. To provide higher-quality recommendations, this study proposes a personalized recommendation model for educational video game resources based on knowledge graphs. Initially, feature extraction is performed alternately on the user and item sides. Subsequently, a hidden Markov model is introduced based on the dual-end neighbor algorithm, optimized to consider the temporal nature of users. Experimental analysis reveals that the designed model achieves a hit rate index value of 0.7989 and a normalized cumulative gain value of 0.6045. Over 89% of users express satisfaction with the recommendations provided by this model. The constructed model efficiently delivers high-quality recommendations of educational video game resources, offering users a more convenient and efficient online experience.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: ï»¿Fuzzy inference systems (FISs) have been developed for many years but the use of FISs for high-dimensional problems is still a challenging task. The most frequently used T-norms for computing the firing strengths are product and minimum operators of which the former is often preferred because of its differentiability. However, for high-dimensional problems, the product T-norm suffers from the numeric underflow problem. Here, we primarily focus on addressing the problem that is associated with the use of the T-norms for designing high-dimensional FISs (HDFISs). For the product T-norm, we construct an HDFIS named HDFIS-prod, which easily escapes from the numeric underflow problem. The main novelty is that we propose an adaptive dimension-dependent membership function (DMF). For the minimum T-norm, an empirical observation led us to develop a mechanism that has the natural ability to deal with super high-dimensional problems, which results in another HDFIS named HDFIS-min. Both HDFIS-prod and HDFIS-min are tested on 18 datasets with feature dimensions varying from 1024 to 120450. The simulation results demonstrate that both of them have competitive performance on handling high-dimensional datasets.
Archivo de copia: Machine learning (ML) is a form of artificial intelligence poised to transform the twenty-first century. Recent rapid progress in its underlying architecture and algorithms, along with the expansion in the size of datasets, has led to increased computational competence across various fields. These include driving vehicles, language translation, chatbots, and surpassing human performance at complex board games such as Go. Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimization. We then summarize the applications of ML to medicine, showcasing recent diagnostic performances, and caveats, in fields such as dermatology, radiology, pathology, and general microscopy.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 87.71%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: ï»¿The main idea of this paper is the substantiation of the methodological approach to the assessment of personnel risks of enterprises based on the application of the fuzzy logic apparatus in order to identify the problems of personnel risk management and provide appropriate recommendations for their solution. The methodological basis of the study is the classic provisions and fundamental works of foreign and domestic scientists, statistical data, the results of our research into the problems of assessing personnel risks of enterprises. The methods of fuzzy set theory, comparative analysis, scientific abstraction, generalization of scientific experience of modern theoretical research, systemcomplex approach were used. The study proposed a methodological approach to assessing the level of personnel risks of an enterprise; numerical experiments were conducted on the basis of a group of construction equipment manufacturers. Analysis of the results of assessing the level of personnel risks of enterprises made it possible to identify the problems of managing personnel risks at enterprises Statement of a mathematical problem: the work considers hierarchical fuzzy data, namely: four groups of indicators for assessing the level of personnel risks (quantitative composition â F1, state of qualifications and intellectual potential â F2, staff turnover â F3, motivational system â F4), each of the indicators has a different number of fuzzy coefficients (there are twelve of them in the current work â vi , i=1Ã·12). Indicators are functions of fuzzy coefficients: F1 = r(v1, v2, v3); F2 = g(v4,v5, v6, v7); F3 = h(v8, v9, v10,); F4=q(v11, v12). As an output variable, there is a functional â an integrated indicator Int = f(F1, F2, F3, F4) of the personnel risk level, which, in turn, is also a fuzzy value. Here, the functions r, g, h, q, f are unknown functions of the given variables. We have expert evaluations of the change in all input data; as a rule, they vary within three terms: Low (I), Medium (G), High (E). Formalized information on each variable can be written as , then for a group of indicators we have: . Using a fuzzy system and performing calculations with its help requires the system to have the following structural elements: membership functions of input and output variables, a rule base, and an output mechanism. These structural elements are the components that will be built when designing a fuzzy system. The built mathematical model and the method of its formalization on the basis of FST make it possible to estimate the level of personnel risk at the enterprise, which enables further substantiation of a set of measures to increase the efficiency of its use. The constructed system of fuzzy logical inference can be considered intelligent as it uses elements of computational intelligence, in particular, the theory of fuzzy sets. The proposed methodological approach to assessing the level of personnel risks of enterprises based on the apparatus of fuzzy logic allows, in contrast to existing ones, to integrate the consideration of both qualitative and quantitative indicators when assessing the level of personnel risks and personnel movement indicators and to significantly increase the efficiency of decision-making under conditions of uncertainty and reduce costs in the event of adverse situations.
Archivo de copia: As emphasized by Stephen Yang in his ICCE 2019 keynote speech (Yang, 2019), precision education presents a new challenge when applying artificial intelligence (AI), machine learning, and learning analytics to enhance teaching quality and learning performance. The aim of precision education is to identify at-risk students as early as possible and provide timely interventions based on teaching and learning experiences (Lu et al., 2018). Building on the central theme of precision education, this special issue advocates for an in-depth dialogue between technology and humanity, offering deeper insights into precision education. For this special issue, thirteen research papers focusing on precision education, AI, machine learning, and learning analytics were exchanged, providing in-depth research experiences concerning various applications, methods, pedagogical models, and environments, fostering a better understanding of AI's application in education.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 88.32%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Smart cities represent the convergence of information and communication technologies (ICT) with urban management to improve the quality of life of city dwellers. In this context, recommender systems, tools that offer personalised suggestions to city dwellers, have emerged as key contributors to this convergence. Their successful application in various areas of city life and their ability to process massive amounts of data generated in urban environments has expedited their status as a crucial technology in the evolution of city planning. Our methodology included reviewing the Web of Science database, resulting in 130 articles that, filtered for relevancy, were reduced to 86. The first stage consisted of carrying out a bibliometric analysis with the objective of analysing structural aspects with the SciMAT tool. Secondly, a systematic literature review was undertaken using the PRISMA 2020 statement. The results illustrated the different processes by which recommendations are filtered in areas such as tourism, health, mobility, and transport. This research is seen as a significant breakthrough that can drive the evolution and efficiency of smart cities, establishing a solid framework for future research in this dynamic field.

Archivo de copia: In scientific disciplines such as biomechanics, genetics, ethology, and neurology, accurate tracking of animal behavior is crucial for studies, often without the use of markers. However, precise stance extraction from constantly shifting backgrounds has posed challenges. Recently, an open-source toolbox utilizing advanced algorithms for human position estimation was unveiled, enabling users to train deep neural networks for accurate monitoring of user-defined features. This revised Python module incorporates new features, including graphical user interfaces (GUIs), performance enhancements, and network refinement through active learning, empowering users to create unique and repeatable analysis pipelines using graphical processing units (GPUs).
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Purpose: The general purpose of the study was to investigate the effectiveness of recommender systems in knowledge discovery.

Methodology: The study adopted a desktop research methodology. Desk research refers to secondary data or that which can be collected without fieldwork. Desk research is basically involved in collecting data from existing resources hence it is often considered a low cost technique as compared to field research, as the main cost is involved in executiveâs time, telephone charges and directories. Thus, the study relied on already published studies, reports and statistics. This secondary data was easily accessed through the online journals and library.

Findings: The findings reveal that there exists a contextual and methodological gap relating to recommender systems in knowledge discovery. The study on the effectiveness of recommender systems in knowledge discovery found that such systems played a pivotal role in facilitating users' exploration of vast information repositories, enabling them to uncover relevant resources and expand their knowledge. It found that recommender systems employing advanced algorithms and personalized techniques demonstrated higher effectiveness in generating relevant recommendations tailored to users' preferences and needs. Additionally, the study highlighted the positive correlation between user engagement metrics and knowledge discovery outcomes, emphasizing the importance of fostering active user participation in the recommendation process. Contextual information was also identified as a crucial factor influencing recommendation effectiveness. Overall, the study underscored the significance of continuous refinement and optimization of recommender system algorithms to enhance knowledge discovery outcomes for users.

Unique Contribution to Theory, Practice and Policy: The Social Learning theory, Information Foraging theory and Cognitive Load theory may be used to anchor future studies on recommender systems in knowledge discovery. The study provided recommendations to enhance the efficacy of such systems. It suggested adopting hybrid recommender systems that combine collaborative and content-based filtering techniques to offer more accurate and diverse recommendations. Additionally, the study emphasized the importance of integrating contextual information into recommendation algorithms to dynamically adjust recommendations based on situational context. Furthermore, it recommended the use of explainable AI techniques to improve transparency and user understanding of recommendation processes. Maximizing user engagement through active participation and feedback was also highlighted as crucial, along with prioritizing recommendation diversity to foster exploration and serendipitous discovery of new knowledge resources.
Archivo de copia: The integration of Artificial Intelligence (AI) is rapidly reshaping various sectors, including education, by improving the learning process, enhancing student outcomes, and streamlining administrative tasks. This study seeks to investigate the application of AI in educational management, analyzing both its benefits and challenges. Employing a systematic review approach, the research examines existing literature on AI in educational management. It is evident from the study that AI offers several advantages, such as enhancing student engagement, personalizing learning experiences, and reducing costs. However, the deployment of AI also presents challenges, including ethical considerations, potential biases, and the need for workforce re-skilling. In conclusion, while AI holds tremendous potential to enhance educational management, its implementation must be approached with careful consideration and attention.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 75.31%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Monte Carlo Tree Search (MCTS) is a pronounced empirical search algorithm for agent decision-making, especially when enhanced by Deep Learning (DL), in mastering board games that were once thought to be unconquerable. However, it does not appear to be as equally successful in the domain of real-time video games, where the simulation time limit for exploration is a crucial factor, since they are generally designed to be played by human users and hence require a significant amount of resources for simulation. We in this paper propose a surrogate-assisted MCTS approach, specifically targeting commercial real-time video games by approximating the result of gameplay with a deep-learning-based surrogate model. The key contribution of our work is that we designed a modified MCTS for video games that are both commercial and processed in real-time. Since commercial video games include considerably more complex and dynamic gameplays to satisfy their market consumers, as opposed to their non-commercial analogs, our work can be regarded as having challenged the domain unattempted by precedent studies. We validated the performance of our method by conducting a comparative experiment with other algorithms, including the traditional MCTS, under the environment of a commercial real-time video game.

Archivo de copia: As of the year 2021, over 30 countries have introduced national artificial intelligence (AI) policy strategies, outlining their plans and expectations regarding the impact of AI across various policy sectors, including education. This analysis delves into the thematic content of 24 such national AI policy strategies, with a specific focus on the role of education within the broader discourse on global AI policy. The findings indicate a notable absence of discussions surrounding the use of AI in education (AIED), despite a strong emphasis on the instrumental value of education in fostering an AI-ready workforce and training AI experts. Furthermore, ethical considerations pertaining to AIED receive limited attention, despite the prevalence of AI ethics discussions within these policy documents. This suggests a lack of mainstream awareness and prioritization of AIED's implications among key decision-makers. In light of these findings, the article proposes leveraging a framework of AI ethics principles to better integrate AIED considerations into policymaking discussions and offers recommendations for AIED scholars to engage more effectively with the policymaking process.
¿Es plagio?: Sí
Tipo de plagio: Desordenar frases
Porcentaje de plagio: 78.79%
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Accurately scientific disciplines, including biomechanics, genetics, ethology, and neurology, it is essential to accurately track the behavior of animals throughout studies, particularly without employing markers. However, it has proven difficult to extract precise stances from backgrounds that are always shifting. Recently, we unveiled an open-source toolset that makes use of a cutting-edge algorithm for estimating human position. With the help of this toolbox, users may train a deep neural network to accurately monitor user-defined features with tracking accuracy that rivals that of human labeling. We have added new features, including as graphical user interfaces (GUIs), efficiency improvements, and network refinement based on active learning, to this revised Python module. In order to help customers create a unique and repeatable analysis pipeline using a graphical processing unit (GPU).

Archivo de copia: Ensuring accurate trajectory tracking in real-world scenarios presents a significant challenge, particularly due to inherent tracking errors. This paper introduces a comprehensive motion planning and control framework designed to address these challenges by coordinating between the motion planning layer and the controller. The framework divides motion space into safe and risky regions, leveraging a repulsive potential field to accommodate tracking errors and ensure safety. By combining global search methods and a novel waypoint set filtering technique, collision-free waypoint sets are generated to facilitate trajectory planning. Additionally, an optimization-based approach minimizes trajectory acceleration, followed by an anti-collision modification to verify and refine the planned trajectory for safety. Through the incorporation of invertible transformation and adaptive compensation mechanisms, the framework effectively limits transient trajectory tracking errors within predefined regions, even in the presence of actuator faults. Overall, this motion planning and control framework ensures safety and reliability by coordinating planning and control levels to address tracking errors and actuator faults.
¿Es plagio?: No
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: With the development of education and technology, teachers have gradually realized that games should not be just a way for students to entertain themselves. Applying games to teaching resources can achieve better teaching outcomes. However, related resources are constantly emerging on the internet. To achieve higher quality recommendations, a personalized recommendation model for educational video game resources based on knowledge graphs is proposed. Firstly, feature extraction is performed alternately on the user side and the item side. Then a hidden Markov model is introduced on the basis of the dual end neighbor algorithm. Considering the temporal nature of the user, the model is optimized. The optimized model takes into account the long-term and short-term preferences of users and mines their potential preferences. Through experimental analysis, the hit rate index value of the designed model reaches 0.7989. The normalized cumulative gain value of the broken line is 0.6045. More than 89Ê% of users are satisfied with the recommendation of this model. The running time is 0.2863Ês. The constructed model can achieve efficient and high-quality recommendation of educational video game resources, providing users with a more convenient and efficient online experience.

Archivo de copia: Recent years have witnessed a convergence between Artificial Intelligence (AI) and neuroscience, particularly in understanding neural processes and developing treatments for neurological disorders. Artificial neural networks and deep learning techniques offer valuable insights into neural processing and brain function, providing new avenues for exploring individual happiness. This study employs a bibliometric analysis of articles from the Scopus database spanning 2013-2023 to evaluate the interaction between neuroscience and happiness through AI advancements. The analysis reveals significant scientific production primarily centered in the United States, United Kingdom, and China. Three distinct clusters emerge from the analysis, highlighting AI applications in predicting happiness, tools for neuroscience research, and the intersection of neuroscience with psychology. Overall, neuroscience research, coupled with AI advancements, contributes to a deeper understanding of mental processes, emotions, and consciousness, particularly in exploring the concept of happiness.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

-----------------------------------------------------
Archivo original: Machine learning (ML) is a form of artificial intelligence which is placed to transform the twenty-first century. Rapid, recent progress in its underlying architecture and algorithms and growth in the size of datasets have led to increasing computer competence across a range of fields. These include driving a vehicle, language translation, chatbots and beyond human performance at complex board games such as Go. Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimisation. We then summarise the applications of ML to medicine. In particular, we showcase recent diagnostic performances, and caveats, in the fields of dermatology, radiology, pathology and general microscopy.
Archivo de copia: This article examines the factors contributing to technological disruptions in electrical systems and identifies inherent drawbacks in the protection and automation of electrical system components. It argues that the transition from analog to digital protection mechanisms has led to decreased reliability in relay protection systems. To mitigate these challenges, the article proposes the integration of fuzzy logic elements into protection devices and automation systems, emphasizing the benefits of utilizing fuzzy logic to identify various types of short circuits. Through modeling exercises and simulations, the article demonstrates the effectiveness of fuzzy logic in identifying and addressing asymmetric short circuits within electrical systems. Additionally, the article presents an algorithm for protection and automation operations leveraging fuzzy logic elements, aimed at reducing the time required to identify damage types and trigger protective measures.
¿Es plagio?: Sí
Tipo de plagio: 
Porcentaje de plagio: %
-----------------------------------------------------

